{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export the trained model in safetensor formats for compatibility with downstream inference engines. First, we'll define some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LightGPT-Small-Base\"\n",
    "checkpoint_path = \"./checkpoints/checkpoint.pt\"\n",
    "lora_path = None  # \"./checkpoints/instruct.pt\"\n",
    "exports_path = \"./exports\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll load the base model checkpoint into memory from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from model import LightGPT\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(checkpoint[\"token_encoding\"])\n",
    "\n",
    "model = LightGPT(**checkpoint[\"model_args\"])\n",
    "\n",
    "state_dict = checkpoint[\"model\"]\n",
    "\n",
    "# Compensate for poorly designed PyTorch compiled state dicts.\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key.replace(\"_orig_mod.\", \"\")] = state_dict.pop(key)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "print(\"Base checkpoint loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load any fine-tuned token embeddings and LoRA checkpoints we wish to incorporate into the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LightGPTInstruct\n",
    "\n",
    "from tiktoken import Encoding\n",
    "\n",
    "if lora_path != None:\n",
    "    checkpoint = torch.load(lora_path, map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "    tokenizer = Encoding(\n",
    "        name=tokenizer.name,\n",
    "        pat_str=tokenizer._pat_str,\n",
    "        mergeable_ranks=tokenizer._mergeable_ranks,\n",
    "        special_tokens=[\n",
    "            *tokenizer._special_tokens,\n",
    "            \"<|im_start|>\",\n",
    "            \"<|im_end|>\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = LightGPTInstruct(model, **checkpoint[\"lora_args\"])\n",
    "\n",
    "    model.model.token_embeddings.load_state_dict(checkpoint[\"token_embeddings\"])\n",
    "    model.load_state_dict(checkpoint[\"lora\"], strict=False)\n",
    "\n",
    "    model.merge_lora_parameters()\n",
    "\n",
    "    print(\"LoRA checkpoint loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's export the model in HuggingFace format so that it can be used with the HuggingFace ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "from transformers.integrations.tiktoken import convert_tiktoken_to_fast\n",
    "from transformers import PreTrainedTokenizerFast, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "from model import LightGPTHuggingFaceConfig, LightGPTHuggingFaceModel\n",
    "\n",
    "hf_path = path.join(exports_path, model_name)\n",
    "\n",
    "# Wait for buggy HuggingFace conversion code to be fixed.\n",
    "#convert_tiktoken_to_fast(tokenizer, hf_path)\n",
    "\n",
    "AutoConfig.register(\"lightgpt\", LightGPTHuggingFaceConfig)\n",
    "AutoModelForCausalLM.register(LightGPTHuggingFaceConfig, LightGPTHuggingFaceModel)\n",
    "\n",
    "LightGPTHuggingFaceConfig.register_for_auto_class()\n",
    "LightGPTHuggingFaceModel.register_for_auto_class(\"AutoModel\")\n",
    "\n",
    "hf_config = LightGPTHuggingFaceConfig(**checkpoint[\"model_args\"])\n",
    "\n",
    "hf_model = LightGPTHuggingFaceModel(hf_config)\n",
    "\n",
    "hf_model.model = torch.compile(hf_model.model)\n",
    "\n",
    "# Compensate for HuggingFace Transformers lack of tied weight support.\n",
    "state_dict = model.state_dict()\n",
    "state_dict = {k:v for k, v in state_dict.items() if \"output_layer\" not in k}\n",
    "\n",
    "hf_model.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "hf_model.save_pretrained(hf_path, state_dict=state_dict)\n",
    "\n",
    "print(f\"Model saved to {hf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll login to HuggingFaceHub and upload the model under our account. Unfortunately, we'll need to upload the safetensors files manually to HuggingFace Hub because the HuggingFace library does not support pushing models with tied weights using Safetensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "hf_model.push_to_hub(model_name, safe_serialization=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
